{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a 500-1â€™000 word tutorial about the F1 score and how to use it in Scikit-learn with an example - you can choose the data and model. The target audience is familiar with Python and the sklearn library: estimators API (ex. fit, score), related tools (ex. pipelines), common classification models (ex. logistic regressions), but not yet with the precision and recall metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine we want to design a diagnostic kit for disease detection. After sampling a drop of blood from your fingertip, the result shown on the device is either 'yes' (positive) or 'no' (negative). However, the kit has a small detection error, and it could happen that the diagnostic is incorrect. So far this would mean there are four possible outcomes:\n",
    "\n",
    "- The person has the disease and the kit correctly diagnoses having the disease --> true positive (TP)\n",
    "- The person does not have the disease and the kit correctly diagnoses not having the disease --> true negative (TN)\n",
    "- The person has the disease and the kit incorrectly diagnoses not having the disease --> false negative (FN)\n",
    "- The person does not have the disease and the kit incorrectly diagnoses having the disease --> false positive (FP)\n",
    "\n",
    "Depending on the disease, your physician might decide to choose a different type of kit:\n",
    "\n",
    "- Incorrectly diagnosing tuberculosis (TB) could mean that you would be free to leave and could fatally spread the disease to other people, and therefore the physician would prefer to have a kit that maximizes true positives and minimizes false negatives. \n",
    "- Incorrectly diagnosing a common cold could mean that you might need further medical examination at a certain cost, and therefore the physician might prefer to have a kit that maximizes true positives and minimizes false positives. \n",
    "\n",
    "This simplified yet useful example introduces two key concepts in classification tasks:\n",
    "- $\\textrm{Recall} = \\frac{TP}{TP+FN} \\rightarrow$ proportion of positive predictions that are actually relevant\n",
    "- $\\textrm{Precision} = \\frac{TP}{TP+FP} \\rightarrow$ proportion of positive predictions that are actually correct\n",
    "\n",
    "These two metrics are interdependent. Let's go back to the TB example and assume that, for the sake public health, we design a kit that always predicts that the person has the disease. This means that we would not miss any case (no false negatives!), and the recall of the kit would be exactly one. However, the prevalence of tuberculosis in Switzerland is 7 cases per 100k inhabitants, which means that we would be sending to hospital a huge number of people that actually do not have TB (false positives!), and hence our precision in this case would be close to zero. The opposite would happen if we design a kit that never misclassifies a patient not having the disease (no false positives!), so the precision of this new kit would be exactly one. However, if out of those 7 actual TB cases we only manage to pick up one with the kit, there would be 6 people in the nature (false negatives!) spreading a potentially fatal disease.\n",
    "\n",
    "To include the influence of precision and recall in a single metric, we can use the F1 score, which is a harmonic average of precision and recall:\n",
    "$$\\textrm{F1 score} = 2 \\times \\frac{\\textrm{precision} \\times \\textrm{recall}}{\\textrm{precision} + \\textrm{recall}}$$\n",
    "\n",
    "From the limit values of precision and recall, we can see that the F1 score can vary from zero to one. This metric is extremely useful in classification tasks with heavily imbalanced classes, where the accuracy metric is not a reliable measure of the predictive power of the model. Think again about the TB case and imagine we design a kit that always predict that the person does not have the disease. According to the prevalence in Switzerland, that would be a prediction accuracy of 99.993% - but yet a very big public health concern...\n",
    "\n",
    "So, take-away key message: you need to choose the most suitabl metric for the problem you are trying to solve! And you can now add F1 score to your collection :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through a specific example using sklearn and the breast cancer Wisconsin (Diagnostic) Data Set that you can find in the UCI Machine Learning Repository:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('~/Desktop/Analytics/wdbc.txt', sep=',', header=None) # read the txt data with pandas\n",
    "input_data.columns = ['ID', 'Diagnosis'] + list(np.arange(1, 31).astype('object')) # rename the columns\n",
    "input_data['target'] = input_data.Diagnosis.map({'B' : 0, 'M' : 1}) # assign the positive class to the malignant tumour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a simple random forest model and calculate the f1_score, precision and accuracy using sklearn. This ensemble model has been chosen because of the class imbalance in the input data.\n",
    "\n",
    "X = input_data.drop(columns=['Diagnosis', 'target']).values\n",
    "y = input_data.target.values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0) # create train and test sets\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=50) # random forest model\n",
    "\n",
    "rf.fit(X_tr, y_tr) # fit the random forest model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random forest model has an F1 score of 0.93 , precision of 0.91 , and recall of 0.95 for the test set\n"
     ]
    }
   ],
   "source": [
    "# Once the model has been fitted, the F1 score, precision and recall can be calculated using sklearn's metric module:\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = rf.predict(X_te) # predict the test classes\n",
    "f1 = f1_score(y_te, y_pred) # calculate the F1 score\n",
    "precision = precision_score(y_te, y_pred) # calculate the precision\n",
    "recall = recall_score(y_te, y_pred) # calculate the recall \n",
    "\n",
    "print('The random forest model has an F1 score of {:.2f}'.format(f1), ', precision of {:.2f}'.format(precision), ', and recall of {:.2f}'.format(recall), 'for the test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
